{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f7f1272",
   "metadata": {},
   "source": [
    "# Title ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37832944",
   "metadata": {},
   "source": [
    "### 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/NudelMaster/deep_learning_hw.git\n",
    "\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7bc385",
   "metadata": {},
   "source": [
    "### 2. Data Loading\n",
    "Loads the data if binary files exist, otherwise automatically downloads and stores in /data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3031b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_dataset = datasets.STL10(root='./data', split='train', download=True)\n",
    "test_dataset = datasets.STL10(root='./data', split='test', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95573de",
   "metadata": {},
   "source": [
    "### 3. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb0037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "labels_map = {\n",
    "    0 : \"Airplane\", 1 : \"Bird\", 2 : \"Car\", 3 : \"Cat\", 4 : \"Deer\",\n",
    "    5 : \"Dog\", 6 : \"Horse\", 7 : \"Monkey\", 8 : \"Ship\", 9 : \"Truck\"\n",
    "}\n",
    "# plot 4 different examples from each class\n",
    "# configure the plot\n",
    "fig, axes = plt.subplots(nrows = 10, ncols = 4, figsize=(10,20))\n",
    "# iterate over 10 classes\n",
    "for i in range(10):\n",
    "    # Filter data for this class\n",
    "    class_i_examples = train_dataset.data[train_dataset.labels == i]\n",
    "\n",
    "    # Pick 4 random indices\n",
    "    idx = np.random.randint(0, class_i_examples.shape[0], 4)\n",
    "\n",
    "    for j in range(4):\n",
    "        img = class_i_examples[idx[j]]\n",
    "        ax = axes[i, j]\n",
    "\n",
    "        # Call the fixed function\n",
    "        ax.imshow(np.transpose(img, (1, 2, 0)))\n",
    "        ax.set_title(labels_map[i])\n",
    "\n",
    "        # Remove axis ticks for cleanliness\n",
    "        ax.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17938681",
   "metadata": {},
   "source": [
    "### Visualize Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ca3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img, _ = train_dataset[0] # Get a PIL image\n",
    "import torchvision.transforms as transforms\n",
    "# Define individual transforms to show them one by one\n",
    "crop_tf = transforms.RandomResizedCrop(64)\n",
    "flip_tf = transforms.RandomHorizontalFlip(p=1.0) # Force flip for the demo\n",
    "color_tf = transforms.ColorJitter(brightness=0.5)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "axes[0].imshow(sample_img); axes[0].set_title(\"Original\")\n",
    "axes[1].imshow(crop_tf(sample_img)); axes[1].set_title(\"Random Crop\")\n",
    "axes[2].imshow(flip_tf(sample_img)); axes[2].set_title(\"Horizontal Flip\")\n",
    "axes[3].imshow(color_tf(sample_img)); axes[3].set_title(\"Color Jitter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84c9d5f",
   "metadata": {},
   "source": [
    "### 4. Prepare Data and configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70fae7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import models\n",
    "import trainer\n",
    "import utils\n",
    "import data_setup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_subset, val_subset, test_subset = data_setup.get_stl10_splits(train_dataset=train_dataset,\n",
    "                                                                  test_dataset=test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43a607",
   "metadata": {},
   "source": [
    "### Test accuracy helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cc322c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model_performance(model, test_dataset, device, num_workers=0):\n",
    "    model.eval()\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=num_workers)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    print(\"Evaluating on Test Set...\")\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Final Accuracy on the 8000 test images: {100 * correct / total:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c610ef45",
   "metadata": {},
   "source": [
    "### 5. Model implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['logreg', 'nn', 'cnn', 'mobilenet']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfb263a",
   "metadata": {},
   "source": [
    "- ### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f69b4f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search for 'logreg' with 36 candidates...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d7388dc58c48f69c4f9ce466bbd79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.01, 'batch_size': 64, 'weight_decay': 0, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 27.1495 | Val Loss: 20.3852 | Val Acc: 23.70%\n",
      "Epoch 10/15 | Train Loss: 25.0535 | Val Loss: 24.9196 | Val Acc: 17.70%\n",
      "Epoch 15/15 | Train Loss: 26.0056 | Val Loss: 20.4947 | Val Acc: 21.70%\n",
      "Best Val Acc: 23.70%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.01, 'batch_size': 64, 'weight_decay': 0.001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 28.5132 | Val Loss: 21.9670 | Val Acc: 21.10%\n",
      "Epoch 10/15 | Train Loss: 23.9146 | Val Loss: 23.2999 | Val Acc: 20.20%\n",
      "Epoch 15/15 | Train Loss: 25.4553 | Val Loss: 19.9964 | Val Acc: 21.30%\n",
      "Best Val Acc: 24.50%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.01, 'batch_size': 64, 'weight_decay': 0.0001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 28.4452 | Val Loss: 21.9144 | Val Acc: 18.10%\n",
      "Epoch 10/15 | Train Loss: 24.7816 | Val Loss: 22.3153 | Val Acc: 22.40%\n",
      "Epoch 15/15 | Train Loss: 24.6144 | Val Loss: 20.3589 | Val Acc: 22.40%\n",
      "Best Val Acc: 23.80%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.01, 'batch_size': 128, 'weight_decay': 0, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 24.6370 | Val Loss: 16.9229 | Val Acc: 21.70%\n",
      "Epoch 10/15 | Train Loss: 15.3089 | Val Loss: 13.7445 | Val Acc: 24.00%\n",
      "Epoch 15/15 | Train Loss: 17.7579 | Val Loss: 20.9139 | Val Acc: 17.70%\n",
      "Best Val Acc: 24.00%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.01, 'batch_size': 128, 'weight_decay': 0.001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 24.6428 | Val Loss: 16.7725 | Val Acc: 21.70%\n",
      "Epoch 10/15 | Train Loss: 15.3935 | Val Loss: 13.1720 | Val Acc: 21.00%\n",
      "Epoch 15/15 | Train Loss: 16.9011 | Val Loss: 15.4919 | Val Acc: 20.00%\n",
      "Best Val Acc: 22.80%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.01, 'batch_size': 128, 'weight_decay': 0.0001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 24.6338 | Val Loss: 16.9230 | Val Acc: 21.70%\n",
      "Epoch 10/15 | Train Loss: 15.3712 | Val Loss: 14.1738 | Val Acc: 22.20%\n",
      "Epoch 15/15 | Train Loss: 16.1579 | Val Loss: 17.3120 | Val Acc: 20.30%\n",
      "Best Val Acc: 23.70%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 64, 'weight_decay': 0, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 3.4553 | Val Loss: 2.9875 | Val Acc: 22.70%\n",
      "Epoch 10/15 | Train Loss: 3.3326 | Val Loss: 3.1895 | Val Acc: 20.30%\n",
      "Epoch 15/15 | Train Loss: 3.3565 | Val Loss: 2.7513 | Val Acc: 23.70%\n",
      "Best Val Acc: 24.90%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 64, 'weight_decay': 0.001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 3.4551 | Val Loss: 2.9871 | Val Acc: 22.70%\n",
      "Epoch 10/15 | Train Loss: 3.3310 | Val Loss: 3.1890 | Val Acc: 20.30%\n",
      "Epoch 15/15 | Train Loss: 3.3545 | Val Loss: 2.7475 | Val Acc: 23.80%\n",
      "Best Val Acc: 25.00%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 64, 'weight_decay': 0.0001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 3.4553 | Val Loss: 2.9875 | Val Acc: 22.70%\n",
      "Epoch 10/15 | Train Loss: 3.3325 | Val Loss: 3.1895 | Val Acc: 20.30%\n",
      "Epoch 15/15 | Train Loss: 3.3563 | Val Loss: 2.7509 | Val Acc: 23.80%\n",
      "Best Val Acc: 24.90%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 128, 'weight_decay': 0, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 3.2740 | Val Loss: 2.7746 | Val Acc: 20.10%\n",
      "Epoch 10/15 | Train Loss: 2.8932 | Val Loss: 2.5870 | Val Acc: 25.40%\n",
      "Epoch 15/15 | Train Loss: 2.8363 | Val Loss: 2.7448 | Val Acc: 22.00%\n",
      "Best Val Acc: 25.40%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 128, 'weight_decay': 0.001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 3.2738 | Val Loss: 2.7746 | Val Acc: 20.10%\n",
      "Epoch 10/15 | Train Loss: 2.8924 | Val Loss: 2.5863 | Val Acc: 25.40%\n",
      "Epoch 15/15 | Train Loss: 2.8353 | Val Loss: 2.7444 | Val Acc: 22.00%\n",
      "Best Val Acc: 25.40%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.001, 'batch_size': 128, 'weight_decay': 0.0001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 3.2740 | Val Loss: 2.7746 | Val Acc: 20.10%\n",
      "Epoch 10/15 | Train Loss: 2.8931 | Val Loss: 2.5869 | Val Acc: 25.40%\n",
      "Epoch 15/15 | Train Loss: 2.8362 | Val Loss: 2.7448 | Val Acc: 22.00%\n",
      "Best Val Acc: 25.40%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.0001, 'batch_size': 64, 'weight_decay': 0, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 2.2028 | Val Loss: 2.0960 | Val Acc: 24.20%\n",
      "Epoch 10/15 | Train Loss: 2.1135 | Val Loss: 2.0577 | Val Acc: 27.00%\n",
      "Epoch 15/15 | Train Loss: 2.1151 | Val Loss: 2.0324 | Val Acc: 28.20%\n",
      "Best Val Acc: 28.90%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.0001, 'batch_size': 64, 'weight_decay': 0.001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 2.2028 | Val Loss: 2.0960 | Val Acc: 24.20%\n",
      "Epoch 10/15 | Train Loss: 2.1134 | Val Loss: 2.0576 | Val Acc: 27.00%\n",
      "Epoch 15/15 | Train Loss: 2.1150 | Val Loss: 2.0323 | Val Acc: 28.20%\n",
      "Best Val Acc: 28.90%\n",
      "\n",
      "Evaluating Config: {'optimizer': 'adam', 'learning_rate': 0.0001, 'batch_size': 64, 'weight_decay': 0.0001, 'epochs': 15}\n",
      "Model logreg built successfully.\n",
      "Starting training for 15 epochs on cuda...\n",
      "Epoch 1/15 | Train Loss: 2.2028 | Val Loss: 2.0960 | Val Acc: 24.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fe9b677ce00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dsi/yefimnu/miniconda3/envs/deep_learning_hw/lib/python3.12/logging/__init__.py\", line 243, in _releaseLock\n",
      "    def _releaseLock():\n",
      "    \n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 826321, 826322) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning_hw/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning_hw/lib/python3.12/multiprocessing/queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      2\u001b[39m logreg_grid = {\n\u001b[32m      3\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m'\u001b[39m: [\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msgd\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m1e-2\u001b[39m, \u001b[32m1e-3\u001b[39m, \u001b[32m1e-4\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m15\u001b[39m]\n\u001b[32m      8\u001b[39m }\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Perform grid search for logistic regression\u001b[39;00m\n\u001b[32m     12\u001b[39m results, best_logreg_config = \\\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_grid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlogreg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogreg_grid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Print top 10 results\u001b[39;00m\n\u001b[32m     22\u001b[39m sorted_results = \u001b[38;5;28msorted\u001b[39m(results, key=\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[33m'\u001b[39m\u001b[33mbest_val_acc\u001b[39m\u001b[33m'\u001b[39m], reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deep_learning_hw/trainer.py:219\u001b[39m, in \u001b[36mrun_grid_search\u001b[39m\u001b[34m(model_name, train_set, val_set, param_grid, device, num_workers)\u001b[39m\n\u001b[32m    216\u001b[39m     current_config[\u001b[33m'\u001b[39m\u001b[33mlearning_rate\u001b[39m\u001b[33m'\u001b[39m] = current_config[\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m final_val_acc = \u001b[38;5;28mmax\u001b[39m(history[\u001b[33m'\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m    222\u001b[39m results.append({\n\u001b[32m    223\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m'\u001b[39m: current_config,\n\u001b[32m    224\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_val_acc\u001b[39m\u001b[33m'\u001b[39m: final_val_acc\n\u001b[32m    225\u001b[39m })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deep_learning_hw/trainer.py:134\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_set, val_set, config, device, num_workers)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStarting training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m epochs on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, num_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# Optimization: Calculate train_acc during training pass to avoid double iteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     train_loss, train_acc_epoch = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     val_loss, val_acc = evaluate_accuracy(model, val_loader, device)\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deep_learning_hw/trainer.py:17\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m     14\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m     15\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning_hw/lib/python3.12/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning_hw/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning_hw/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/deep_learning_hw/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1288\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1287\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1289\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 826321, 826322) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# Define hyperparameter grid for logistic regression model\n",
    "logreg_grid = {\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'batch_size': [64, 128],\n",
    "    'weight_decay': [0, 1e-3, 1e-4],\n",
    "    'epochs': [15]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search for logistic regression\n",
    "results, best_logreg_config = \\\n",
    "    trainer.run_grid_search(model_name='logreg', \\\n",
    "    train_set = train_subset,\\\n",
    "    val_set = val_subset, \\\n",
    "    param_grid = logreg_grid, \\\n",
    "    device=device, num_workers=8)\n",
    "\n",
    "\n",
    "\n",
    "# Print top 10 results\n",
    "sorted_results = sorted(results, key=lambda x: x['best_val_acc'], reverse=True)\n",
    "print(f\"Top 10 logreg Configurations:\")\n",
    "for i, res in enumerate(sorted_results[:10]):\n",
    "    print(f\"Rank {i+1}: Val Acc: {res['best_val_acc']:.2f}% | Config: {res['params']}\")\n",
    "\n",
    "\n",
    "\n",
    "# Run best logistic regression model\n",
    "best_logreg_config['epochs'] = 100\n",
    "best_logreg_config['patience'] = 10\n",
    "best_logreg_config['use_scheduler'] = True\n",
    "\n",
    "logistic_regression = models.get_model(model_name='logreg', num_classes=10).to(device)\n",
    "\n",
    "\n",
    "logreg_history = \\\n",
    "    trainer.train_model(model = logistic_regression, \\\n",
    "    train_set=train_subset, \\\n",
    "    val_set=val_subset, \\\n",
    "    config = best_logreg_config,\\\n",
    "    device=device, num_workers=8)\n",
    "\n",
    "\n",
    "# Visualize training history\n",
    "utils.plot_training_history(logreg_history, title=\"Logistic Regression Training History\")\n",
    "\n",
    "\n",
    "# Test logistic regression model\n",
    "test_model_performance(logistic_regression, test_subset, device, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7f45e",
   "metadata": {},
   "source": [
    "- ### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913fdb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for neural network model\n",
    "nn_grid = {\n",
    "    'hidden_dim': [512, 1024],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'batch_size': [32, 64],\n",
    "    'weight_decay': [0, 1e-3, 1e-4],\n",
    "    'epochs': [15]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search for neural network\n",
    "results, best_nn_config = \\\n",
    "    trainer.run_grid_search('nn', \\\n",
    "    train_set = train_subset, \\\n",
    "    val_set = val_subset,\\\n",
    "    param_grid = nn_grid, \\\n",
    "    device=device, num_workers=8)\n",
    "\n",
    "\n",
    "# Print top 10 results\n",
    "sorted_results = sorted(results, key=lambda x: x['best_val_acc'], reverse=True)\n",
    "print(f\"Top 10 nn Configurations:\")\n",
    "for i, res in enumerate(sorted_results[:10]):\n",
    "    print(f\"Rank {i+1}: Val Acc: {res['best_val_acc']:.2f}% | Config: {res['params']}\")\n",
    "\n",
    "\n",
    "\n",
    "# Run best neural network model\n",
    "best_nn_config['epochs'] = 120\n",
    "best_nn_config['patience'] = 10\n",
    "best_nn_config['use_scheduler'] = True\n",
    "\n",
    "\n",
    "nn = models.get_model('nn', num_classes=10, hidden_dim=best_nn_config.get('hidden_dim', 512)).to(device)\n",
    "\n",
    "nn_model, nn_history = trainer.train_model(\n",
    "    nn,\n",
    "    train_set=train_subset,\n",
    "    val_set=val_subset,\n",
    "    config=best_nn_config,\n",
    "    device=device,\n",
    "    num_workers=8\n",
    ")\n",
    "\n",
    "\n",
    "# Visualize training history\n",
    "utils.plot_training_history(nn_history, title=\"Neural Network Training History\")\n",
    "# Test neural network model\n",
    "test_model_performance(nn_model, test_subset, device, num_workers=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc0d3e",
   "metadata": {},
   "source": [
    "- ### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdee4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for convolutional neural network model\n",
    "cnn_grid = {\n",
    "    'base_width': [16, 32],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'batch_size': [32, 64],\n",
    "    'weight_decay': [1e-3, 1e-4],\n",
    "    'learning_rate': [1e-2, 1e-3, 5e-4],\n",
    "    'epochs': [15]\n",
    "}\n",
    "\n",
    "\n",
    "# Perform grid search for convolutional neural network\n",
    "results, best_cnn_config = \\\n",
    "    trainer.run_grid_search('cnn', \\\n",
    "    train_set = train_subset, \\\n",
    "    val_set = val_subset, \\\n",
    "    param_grid = cnn_grid, device=device, \\\n",
    "    num_workers=8)\n",
    "\n",
    "\n",
    "# Print top 10 results\n",
    "sorted_results = sorted(results, key=lambda x: x['best_val_acc'], reverse=True)\n",
    "print(f\"Top 10 CNN Configurations:\")\n",
    "for i, res in enumerate(sorted_results[:10]):\n",
    "    print(f\"Rank {i+1}: Val Acc: {res['best_val_acc']:.2f}% | Config: {res['params']}\")\n",
    "\n",
    "\n",
    "# Run best convolutional neural network model\n",
    "best_cnn_config['epochs'] = 150\n",
    "best_cnn_config['patience'] = 20\n",
    "best_cnn_config['use_scheduler'] = True\n",
    "\n",
    "cnn = models.get_model('cnn', num_classes=10, **best_cnn_config).to(device)\n",
    "\n",
    "cnn_history = \\\n",
    "    trainer.train_model(cnn, \\\n",
    "    train_set = train_subset, \\\n",
    "    val_set = val_subset, \\\n",
    "    config = best_cnn_config, \\\n",
    "    device=device, num_workers=8)\n",
    "\n",
    "\n",
    "# Plot training history for the best CNN model\n",
    "utils.plot_training_history(cnn_history)\n",
    "\n",
    "\n",
    "\n",
    "# Test model performance\n",
    "test_model_performance(cnn, test_subset, device, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bbe34d",
   "metadata": {},
   "source": [
    " - ### MobileNet as Feature Extraction with Frozen weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d67d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for MobileNet model with frozen backbone\n",
    "mobilenet_frozen_grid = {\n",
    "    'freeze_backbone': [True],\n",
    "    'weight_decay': [1e-3, 1e-4],\n",
    "    'hidden_dim': [256, 512],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'batch_size': [32,64],\n",
    "    'epochs': [15]\n",
    "}\n",
    "\n",
    "# Perform grid search for MobileNet (frozen)\n",
    "results, best_mobile_net_frozen_config = trainer.run_grid_search('mobilenet', \\\n",
    "    train_set = train_subset, \\\n",
    "    val_set = val_subset, \\\n",
    "    param_grid = mobilenet_frozen_grid,\\\n",
    "    device=device, \\\n",
    "    num_workers=8)\n",
    "\n",
    "\n",
    "# Print top 10 results\n",
    "print(\"Top 10 MobileNet (frozen) Configurations:\")\n",
    "sorted_results = sorted(results, key=lambda x: x['best_val_acc'], reverse=True)\n",
    "for i, res in enumerate(sorted_results[:10]):\n",
    "    print(f\"Rank {i+1}: Val Acc: {res['best_val_acc']:.2f}% | Config: {res['params']}\")\n",
    "\n",
    "# Run best MobileNet (frozen) model\n",
    "\n",
    "best_mobile_net_frozen_config['epochs'] = 100\n",
    "best_mobile_net_frozen_config['patience'] = 20\n",
    "best_mobile_net_frozen_config['use_scheduler'] = True\n",
    "\n",
    "\n",
    "mobile_net_frozen = \\\n",
    "    models.get_model('mobilenet', num_classes=10, **best_mobile_net_frozen_config).to(device)\n",
    "\n",
    "\n",
    "mobile_net_frozen_history = \\\n",
    "    trainer.train_model(mobile_net_frozen, \\\n",
    "    train_set = train_subset, \\\n",
    "    val_set = val_subset, \\\n",
    "    config = best_mobile_net_frozen_config, \\\n",
    "    device=device, num_workers=8)\n",
    "\n",
    "\n",
    "# Plot training history for the best MobileNet (frozen) model\n",
    "utils.plot_training_history(mobile_net_frozen_history, title=\"MobileNet (frozen) Training History\")\n",
    "\n",
    "# Test MobileNet (frozen) model\n",
    "test_model_performance(mobile_net_frozen, test_subset, device, num_workers=8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642ab6fb",
   "metadata": {},
   "source": [
    "- ### Mobile Net finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dfb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid for MobileNet model (fine-tuned)\n",
    "mobilenet_grid = {\n",
    "    'freeze_backbone': [False],\n",
    "    'weight_decay': [1e-3, 1e-4],\n",
    "    'hidden_dim': [256, 512],\n",
    "    'optimizer': ['adam', 'sgd'],\n",
    "    'learning_rate': [1e-2, 1e-3, 1e-4],\n",
    "    'batch_size': [32,64],\n",
    "    'epochs': [15]\n",
    "}\n",
    "\n",
    "# Run grid search for MobileNet (fine-tuned)\n",
    "results, best_mobile_net_config = \\\n",
    "    trainer.run_grid_search('mobilenet', \\\n",
    "    train_set = train_subset, \\\n",
    "    val_set = val_subset, \\\n",
    "    param_grid = mobilenet_grid, \\\n",
    "    device=device, \\\n",
    "    num_workers=8)\n",
    "\n",
    "# Print top 10 results\n",
    "print(\"Top 10 MobileNet (fine-tuned) Configurations:\")\n",
    "sorted_results = sorted(results, key=lambda x: x['best_val_acc'], reverse=True)\n",
    "for i, res in enumerate(sorted_results[:10]):\n",
    "    print(f\"Rank {i+1}: Val Acc: {res['best_val_acc']:.2f}% | Config: {res['params']}\")\n",
    "\n",
    "# Run best MobileNet (fine-tuned) model\n",
    "\n",
    "best_mobile_net_config['epochs'] = 100\n",
    "best_mobile_net_config['patience'] = 20\n",
    "best_mobile_net_config['use_scheduler'] = True\n",
    "\n",
    "\n",
    "mobile_net_fine_tuned = models.get_model('mobilenet', num_classes=10, **best_mobile_net_config).to(device)\n",
    "\n",
    "\n",
    "mobile_net_fine_tuned_history = \\\n",
    "    trainer.train_model(mobile_net_fine_tuned, \\\n",
    "    train_set = train_subset, \\\n",
    "    val_set = val_subset, \\\n",
    "    config = best_mobile_net_config,\\\n",
    "    device=device, num_workers=8)\n",
    "\n",
    "# Plot training history for the best MobileNet (fine-tuned) model\n",
    "utils.plot_training_history(mobile_net_fine_tuned_history)\n",
    "\n",
    "# Test MobileNet (fine-tuned) model\n",
    "test_model_performance(mobile_net_fine_tuned, test_subset, device, num_workers=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
